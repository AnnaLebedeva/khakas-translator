{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050219af",
   "metadata": {},
   "source": [
    "0. Cleaning files from unnesessary symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396fad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsed -i 's/^[[:blank:]●]*//' khakas_ru.txt\n",
    "! gsed -i 's/^[[:blank:]●]*//' khakas_kh.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f088efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! paste -d $'\\t' khakas_ru.txt khakas_kh.txt | shuf | awk -F $'\\t' '{print $1 > \"khakas_shuffed.ru\" ; print $2 > \"khakas_shuffed.kh\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5d808",
   "metadata": {},
   "source": [
    "1. Splitting Chuvash and Khakas data in train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1000013 1M_shuffed.ru > train_parent.ru\n",
    "! tail -n +1000014 1M_shuffed.ru > val_parent.ru\n",
    "! head -n 1000013 1M_shuffed.chv > train_parent.chv\n",
    "! tail -n +1000014 1M_shuffed.chv > val_parent.chv\n",
    "\n",
    "! head -n 38523 khakas_shuffed.ru > train_val_child.ru\n",
    "! tail -n +38524 khakas_shuffed.ru > test_child.ru\n",
    "! head -n 37523 train_val_child.ru > train_child.ru\n",
    "! tail -n +37524 train_val_child.ru > val_child.ru\n",
    "\n",
    "! head -n 38523 khakas_shuffed.kh > train_val_child.kh\n",
    "! tail -n +38524 khakas_shuffed.kh > test_child.kh\n",
    "! head -n 37523 train_val_child.kh > train_child.kh\n",
    "! tail -n +37524 train_val_child.kh > val_child.kh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea7ea1",
   "metadata": {},
   "source": [
    "2. After I filtered the files, I need to duplicate KH data 25 times, so that KH data is well represented in the vocab, and the KH tokens don't appear too rare to have a significant enough place in the vocab. I did it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639da8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! perl -0777pe '$_=$_ x 26' train_child.kh > 26train_child.kh\n",
    "! perl -0777pe '$_=$_ x 26' train_child.ru > 26train_child.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat train_parent.ru 26train_child.ru > train_vocab.ru\n",
    "! cat train_parent.chv 26train_child.kh > train_vocab.chvkh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f6ff3",
   "metadata": {},
   "source": [
    "Then I concatenated all the files that I wanted to use for building the vocabularies the following way and got the combined files that I will later use in subword-nmt for building the vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5bc0e",
   "metadata": {},
   "source": [
    "First we perform learn-bpe operation on our training files. The parameters of this operation are taken from the article. After that we perform apply-bpe operation on source and target files separately to get two separate vocabs:\n",
    "\n",
    "```python\n",
    "# learn BPE on concatenation of train files and extract vocabs \n",
    "cat train_vocab.ru train_vocab.chvkh | subword-nmt learn-bpe -s 20000 -o codes --min-frequency 2 --total-symbols\n",
    "\n",
    "subword-nmt apply-bpe -c codes < train_vocab.ru | subword-nmt get-vocab > vocab_file.ru\n",
    "subword-nmt apply-bpe -c codes < train_vocab.chvkh | subword-nmt get-vocab > vocab_file.chvkh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a9b37",
   "metadata": {},
   "source": [
    "We will explain the BPE operations in a separate notebook, here we will proceed with building the vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c2228",
   "metadata": {},
   "source": [
    "So after getting the vocabs as a result of subword-nmt apply operation, we use the following script to make these vocabs compatible with sockeye model. The script is in the file subword-nmt_vocab2sockeye_vocab.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741ead2",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env  python3\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def getArgs():\n",
    "    usage=\"subword-nmt_vocab2sockeye_vocab.py  subword_nmt_vocab [sockeye_vocab_json]\"\n",
    "    help = ''\n",
    "    parser = ArgumentParser(usage=usage, description=help, add_help=True)\n",
    "    parser.add_argument(\"vocab_filename\", type=str, help=\"Subword-nmt's vocabulary filename\")\n",
    "    parser.add_argument(\"vocab_json\",\n",
    "         nargs   = '?',\n",
    "         type    = lambda f: open(f, mode='w', encoding='UTF-8'),\n",
    "         default = sys.stdout,\n",
    "         help    = \"output file [sys.stdout]\")\n",
    "    parser.add_argument(\"--add_bt_tag\",\n",
    "         dest    = \"add_bt_tag\",\n",
    "         action  = 'store_true',\n",
    "         default = False,\n",
    "         help    = \"Also include BT tag in vocab [%(default)s]\")\n",
    "    parser.add_argument(\"--add_generic_tags\",\n",
    "         dest    = \"add_generic_tags\",\n",
    "         action  = 'store_true',\n",
    "         default = False,\n",
    "         help    = \"Also include BT tag in vocab [%(default)s]\")\n",
    "    parser.add_argument(\"--add_glossaries\",\n",
    "         dest    = \"add_glossaries\",\n",
    "         nargs   = '?',\n",
    "         default = None,\n",
    "         help    = \"Also include the glossaries in vocab [%(default)s]\")\n",
    "\n",
    "\n",
    "    cmd_args = parser.parse_args()\n",
    "\n",
    "    return cmd_args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = getArgs()\n",
    "    vocab = dict({\n",
    "        \"<pad>\": 0,\n",
    "        \"<unk>\": 1,\n",
    "        \"<s>\": 2,\n",
    "        \"</s>\": 3,\n",
    "        })\n",
    "\n",
    "    if args.add_bt_tag:\n",
    "        vocab.update({'<BT>': len(vocab)})\n",
    "\n",
    "    if args.add_generic_tags:\n",
    "        vocab.update({ f'<TAG{i:02}>' : i+len(vocab)-1 for i in range(1, 26) })\n",
    "\n",
    "    with open(args.vocab_filename, mode='r', encoding='UTF-8') as f:\n",
    "        vocab.update({ k: v for v, k in enumerate(map(lambda l: l.strip().split()[0], f.readlines()), start=len(vocab.keys())) })\n",
    "\n",
    "    if args.add_glossaries is not None:\n",
    "        last_token_id = len(vocab.keys())\n",
    "        with open(args.add_glossaries, mode='rt') as f:\n",
    "            for token in map(str.strip, f.readlines()):\n",
    "                if token != '' and token not in vocab:\n",
    "                    vocab[token] = last_token_id\n",
    "                    last_token_id += 1\n",
    "\n",
    "    json.dump(vocab, args.vocab_json, indent=2, ensure_ascii=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9e051",
   "metadata": {},
   "source": [
    "This code adds all the special tags and turns the vocabs into json files suitable for sockeye model. Here is how we apply it for our vocabs:\n",
    "```python\n",
    "# adding tags and creating sockeye-compatible vocabs\n",
    "python subword-nmt_vocab2sockeye_vocab_adj.py --add_bt_tag --add_generic_tags --add_glossaries glossaries_file.txt vocab_file.ru > augmented_vocab_ru.json\n",
    "python subword-nmt_vocab2sockeye_vocab_adj.py --add_bt_tag --add_generic_tags --add_glossaries glossaries_file.txt vocab_file.chvkh > augmented_vocab_chvkh.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dabce",
   "metadata": {},
   "source": [
    "We will use these vocabs as parameters to our sockeye model.  \n",
    "In the next chapter we will talk about how we make actual BPE files for parent and child models separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e557b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
